{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving environment: failed\n",
      "\n",
      "NotWritableError: The current user does not have write permissions to a required path.\n",
      "  path: /home/ubuntu/anaconda2/pkgs/urls.txt\n",
      "  uid: 1000\n",
      "  gid: 1000\n",
      "\n",
      "If you feel that permissions on this path are set incorrectly, you can manually\n",
      "change them by executing\n",
      "\n",
      "  $ sudo chown 1000:1000 /home/ubuntu/anaconda2/pkgs/urls.txt\n",
      "\n",
      "In general, it's not advisable to use 'sudo conda'.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda update conda\n",
    "#pip3 install --user --upgrade tensorflow\n",
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATIENT/TEST INFORMATION:\n",
      "Indication\n",
      "Left ventricular function. Pericardial effusion. Right ventricular function.\n",
      "Height\n",
      "(in) 71\n",
      "Weight (lb)\n",
      "190\n",
      "BSA (m2)\n",
      "2.07 m2\n",
      "BP (mm Hg)\n",
      "108/56\n",
      "HR (bpm)\n",
      "84\n",
      "Status\n",
      "Inpatient\n",
      "Date/Time\n",
      "[**2150-4-18**] at 12:36\n",
      "Test\n",
      "Portable TTE (Complete)\n",
      "Doppler\n",
      "Full Doppler and color Doppler\n",
      "Contrast\n",
      "None\n",
      "Technical Quality\n",
      "Adequate\n",
      "\n",
      "\n",
      "INTERPRETATION:\n",
      "\n",
      "Findings:\n",
      "\n",
      "LEFT ATRIUM\n",
      "Dilated LA.\n",
      "\n",
      "RIGHT ATRIUM/INTERATRIAL SEPTUM\n",
      "Mildly dilated RA.\n",
      "\n",
      "LEFT VENTRICLE\n",
      "Normal LV wall thickness and cavity size. Hyperdynamic LVEF\n",
      ">75%. No resting LVOT gradient.\n",
      "\n",
      "RIGHT VENTRICLE\n",
      "Normal RV chamber size and free wall motion.\n",
      "\n",
      "AORTA\n",
      "Normal aortic diameter at the sinus level. Normal ascending aorta\n",
      "diameter.\n",
      "\n",
      "AORTIC VALVE\n",
      "Mildly thickened aortic valve leaflets (3). No AS. No AR.\n",
      "\n",
      "MITRAL VALVE\n",
      "Mildly thickened mitral valve leaflets. Mild (1+) MR. Normal LV\n",
      "inflow pattern for age.\n",
      "\n",
      "TRICUSPID VALVE\n",
      "Normal tricuspid valve leaflets. Moderate PA systolic\n",
      "hypertension.\n",
      "\n",
      "PERICARDIUM\n",
      "No pericardial effusion.\n",
      "\n",
      "Conclusions:\n",
      "The left atrium is dilated. Left ventricular wall thicknesses and cavity size\n",
      "are normal. Left ventricular systolic function is hyperdynamic (EF>75%). Right\n",
      "ventricular chamber size and free wall motion are normal. The aortic valve\n",
      "leaflets (3) are mildly thickened but aortic stenosis is not present. No\n",
      "aortic regurgitation is seen. The mitral valve leaflets are mildly thickened.\n",
      "Mild (1+) mitral regurgitation is seen. There is moderate pulmonary artery\n",
      "systolic hypertension. There is no pericardial effusion.\n",
      "\n",
      "IMPRESSION\n",
      "Hyperdynamic LV systolic function. Mild mitral regurgitation.\n",
      "Moderate pulmonary artery systolic hypertension.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "notes = pd.read_csv('demo.csv')\n",
    "\n",
    "notes = notes.groupby('HADM_ID')\n",
    "yo = 0\n",
    "for key, val in notes:\n",
    "    k = val.iloc[0]['TEXT']\n",
    "    k = k.split(': ')\n",
    "    [print(i) for i in k]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>CHARTDATE</th>\n",
       "      <th>NOTE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16928</th>\n",
       "      <td>54610</td>\n",
       "      <td>100003.0</td>\n",
       "      <td>2150-04-18</td>\n",
       "      <td>The left atrium is dilated. Left ventricular w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       SUBJECT_ID   HADM_ID   CHARTDATE  \\\n",
       "16928       54610  100003.0  2150-04-18   \n",
       "\n",
       "                                                    NOTE  \n",
       "16928  The left atrium is dilated. Left ventricular w...  "
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hey = preprocess_notes(ex, 500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>CHARTDATE</th>\n",
       "      <th>CHARTTIME</th>\n",
       "      <th>STORETIME</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>CGID</th>\n",
       "      <th>ISERROR</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>NOTE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16928</th>\n",
       "      <td>76581</td>\n",
       "      <td>76338</td>\n",
       "      <td>54610</td>\n",
       "      <td>100003.0</td>\n",
       "      <td>2150-04-18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Echo</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PATIENT/TEST INFORMATION:\\nIndication: Left ve...</td>\n",
       "      <td>apo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  ROW_ID  SUBJECT_ID   HADM_ID   CHARTDATE  CHARTTIME  \\\n",
       "16928       76581   76338       54610  100003.0  2150-04-18        NaN   \n",
       "\n",
       "       STORETIME CATEGORY DESCRIPTION  CGID  ISERROR  \\\n",
       "16928        NaN     Echo      Report   NaN      NaN   \n",
       "\n",
       "                                                    TEXT NOTE  \n",
       "16928  PATIENT/TEST INFORMATION:\\nIndication: Left ve...  apo  "
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_notes(vst, max_len):\n",
    "    vst = vst.sort_values('CHARTDATE')  #arrange by day1,day2, day3..\n",
    "    #input VST is a dataframe of all rows in each individual visit\n",
    "    # return a new series with shorter sentences but corresponds to each day during this visit\n",
    "    df = vst['TEXT'] ##extract \"TEXT\" column as a series\n",
    "    df = df.str.replace('\\n', ' ')\n",
    "    df = df.str.replace('  ', ' ')\n",
    "    df = df.str.replace('\\*+', '')\n",
    "    df = df.str.replace('\\d+', '')\n",
    "    df = df.str.strip(\"[]\")\n",
    "    newcol = []\n",
    "    for row in df:\n",
    "        short = ''\n",
    "        row = row.split(': ')\n",
    "        row.sort(key=len)\n",
    "        while len(short) < max_len:\n",
    "            short += row.pop()\n",
    "        short = short[0:max_len]\n",
    "        newcol.append(short)\n",
    "    newcol = pd.DataFrame({'NOTE': newcol}) \n",
    "    \n",
    "    result = vst[['SUBJECT_ID', 'HADM_ID', 'CHARTDATE']]\n",
    "    newcol= newcol.set_index(result.index)\n",
    "    \n",
    "    #newcol.apply(lambda x:str(x))\n",
    "    #print(type(newcol.iloc[0]))\n",
    "    result = pd.concat([result, newcol], axis=1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(visits[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "work = pd.read_csv('processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "work.head()\n",
    "strArray = work[\"NOTE\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(sentence):  #work on each string/row of note\n",
    "    outF = open(\"prepinput.txt\", \"w\")\n",
    "    outF.write(sentence)\n",
    "    outF.close()\n",
    "    \n",
    "    out = open('prepoutput.jsonl', 'w')\n",
    "    \n",
    "    os.system('python3 ~/payload/bioBertPart/biobert-master/bioextract_features.py \\\n",
    "    --input_file=~/payload/bioBertPart/prepinput.txt \\\n",
    "    --vocab_file=~/payload/bioBertPart/pubmed_pmc_470k/vocab.txt \\\n",
    "    --bert_config_file=~/payload/bioBertPart/pubmed_pmc_470k/bert_config.json \\\n",
    "    --init_checkpoint=~/payload/bioBertPart/pubmed_pmc_470k/biobert_model.ckpt \\\n",
    "    --output_file=~/payload/bioBertPart/prepoutput.jsonl')\n",
    "    \n",
    "    with open('prepoutput.jsonl') as f:\n",
    "        d = json.load(f)\n",
    "    os.system('rm prepinput.txt')\n",
    "    os.system('rm prepoutput.txt')\n",
    "    \n",
    "    sentence_vector = []\n",
    "    for i in range (1, len(d['features'])-1):\n",
    "        sentence_vector = sentence_vector + d['features'][i]['layers'][0]['values']\n",
    "        \n",
    "    number_of_tokens = len(d['features']) - 2\n",
    "    for elem in sentence_vector:\n",
    "        elem = elem/number_of_tokens\n",
    "    return sentence_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-2ff1cf2b8cc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstce\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstrArray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-cd24a02c23a9>\u001b[0m in \u001b[0;36mget_embedding\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'prepoutput.jsonl'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rm prepinput.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rm prepoutput.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m         parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \"\"\"\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "embedding = []\n",
    "for stce in strArray:\n",
    "    embedding.append(get_embedding(stce))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding.to_csv(r'embedding.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
